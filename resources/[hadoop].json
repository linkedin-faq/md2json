[{"question":" Partitioner controls the partitioning of what data?","options":[{"text":"final keys","correct":false},{"text":"final values","correct":false},{"text":"intermediate keys","correct":true},{"text":"intermediate values","correct":false}],"illustrator":""},{"question":" SQL Windowing functions are implemented in Hive using which keywords?","options":[{"text":"UNION DISTINCT, RANK","correct":false},{"text":"OVER, RANK","correct":true},{"text":"OVER, EXCEPT","correct":false},{"text":"UNION DISTINCT, RANK","correct":false}],"illustrator":""},{"question":" Rather than adding a Secondary Sort to a slow Reduce job, it is Hadoop best practice to perform which optimization?","options":[{"text":"Add a partitioned shuffle to the Map job.","correct":false},{"text":"Add a partitioned shuffle to the Reduce job.","correct":true},{"text":"Break the Reduce job into multiple, chained Reduce jobs.","correct":false},{"text":"Break the Reduce job into multiple, chained Map jobs.","correct":false}],"illustrator":""},{"question":" Hadoop Auth enforces authentication on protected resources. Once authentication has been established, it sets what type of authenticating cookie?","options":[{"text":"encrypted HTTP","correct":false},{"text":"unsigned HTTP","correct":false},{"text":"compressed HTTP","correct":false},{"text":"signed HTTP","correct":true}],"illustrator":""},{"question":" MapReduce jobs can be written in which language?","options":[{"text":"Java or Python","correct":true},{"text":"SQL only","correct":false},{"text":"SQL or Java","correct":false},{"text":"Python or SQL","correct":false}],"illustrator":""},{"question":" To perform local aggregation of the intermediate outputs, MapReduce users can optionally specify which object?","options":[{"text":"Reducer","correct":false},{"text":"Combiner","correct":true},{"text":"Mapper","correct":false},{"text":"Counter","correct":false}],"illustrator":""},{"question":" To verify job status, look for the value `___` in the `___`.","options":[{"text":"SUCCEEDED; syslog","correct":false},{"text":"SUCCEEDED; stdout","correct":true},{"text":"DONE; syslog","correct":false},{"text":"DONE; stdout","correct":false}],"illustrator":""},{"question":" Which line of code implements a Reducer method in MapReduce 2.0?","options":[{"text":"public void reduce(Text key, Iterator<IntWritable> values, Context context){…}","correct":true},{"text":"public static void reduce(Text key, IntWritable[] values, Context context){…}","correct":false},{"text":"public static void reduce(Text key, Iterator<IntWritable> values, Context context){…}","correct":false},{"text":"public void reduce(Text key, IntWritable[] values, Context context){…}","correct":false}],"illustrator":""},{"question":" To get the total number of mapped input records in a map job task, you should review the value of which counter?","options":[{"text":"FileInputFormatCounter","correct":false},{"text":"FileSystemCounter","correct":false},{"text":"JobCounter","correct":false},{"text":"TaskCounter (NOT SURE)","correct":true}],"illustrator":""},{"question":". Hadoop Core supports which CAP capabilities?","options":[{"text":"A, P","correct":true},{"text":"C, A","correct":false},{"text":"C, P","correct":false},{"text":"C, A, P","correct":false}],"illustrator":""},{"question":". What are the primary phases of a Reducer?","options":[{"text":"combine, map, and reduce","correct":false},{"text":"shuffle, sort, and reduce","correct":true},{"text":"reduce, sort, and combine","correct":false},{"text":"map, sort, and combine","correct":false}],"illustrator":""},{"question":". To set up Hadoop workflow with synchronization of data between jobs that process tasks both on disk and in memory, use the `___` service, which is `___`.","options":[{"text":"Oozie; open source","correct":false},{"text":"Oozie; commercial software","correct":false},{"text":"Zookeeper; commercial software","correct":false},{"text":"Zookeeper; open source","correct":true}],"illustrator":""},{"question":". For high availability, use multiple nodes of which type?","options":[{"text":"data","correct":false},{"text":"name","correct":true},{"text":"memory","correct":false},{"text":"worker","correct":false}],"illustrator":""},{"question":". DataNode supports which type of drives?","options":[{"text":"hot swappable","correct":true},{"text":"cold swappable","correct":false},{"text":"warm swappable","correct":false},{"text":"non-swappable","correct":false}],"illustrator":""},{"question":". Which method is used to implement Spark jobs?","options":[{"text":"on disk of all workers","correct":false},{"text":"on disk of the master node","correct":false},{"text":"in memory of the master node","correct":false},{"text":"in memory of all workers","correct":true}],"illustrator":""},{"question":". In a MapReduce job, where does the map() function run?","options":[{"text":"on the reducer nodes of the cluster","correct":false},{"text":"on the data nodes of the cluster (NOT SURE)","correct":true},{"text":"on the master node of the cluster","correct":false},{"text":"on every node of the cluster","correct":false}],"illustrator":""},{"question":". To reference a master file for lookups during Mapping, what type of cache should be used?","options":[{"text":"distributed cache","correct":true},{"text":"local cache","correct":false},{"text":"partitioned cache","correct":false},{"text":"cluster cache","correct":false}],"illustrator":""},{"question":". Skip bad records provides an option where a certain set of bad input records can be skipped when processing what type of data?","options":[{"text":"cache inputs","correct":false},{"text":"reducer inputs","correct":false},{"text":"intermediate values","correct":false},{"text":"map inputs","correct":true}],"illustrator":""},{"question":". Which command imports data to Hadoop from a MySQL database?","options":[{"text":"spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --warehouse-dir user/hue/oozie/deployments/spark","correct":false},{"text":"sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --warehouse-dir user/hue/oozie/deployments/sqoop","correct":false},{"text":"sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --password sqoop --warehouse-dir user/hue/oozie/deployments/sqoop","correct":true},{"text":"spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --password spark --warehouse-dir user/hue/oozie/deployments/spark","correct":false}],"illustrator":""},{"question":". In what form is Reducer output presented?","options":[{"text":"compressed (NOT SURE)","correct":true},{"text":"sorted","correct":false},{"text":"not sorted","correct":false},{"text":"encrypted","correct":false}],"illustrator":""},{"question":". Which library should be used to unit test MapReduce code?","options":[{"text":"JUnit","correct":false},{"text":"XUnit","correct":false},{"text":"MRUnit","correct":true},{"text":"HadoopUnit","correct":false}],"illustrator":""},{"question":". If you started the NameNode, then which kind of user must you be?","options":[{"text":"hadoop-user","correct":false},{"text":"super-user","correct":true},{"text":"node-user","correct":false},{"text":"admin-user","correct":false}],"illustrator":""},{"question":". State \\_ between the JVMs in a MapReduce job","options":[{"text":"can be configured to be shared","correct":false},{"text":"is partially shared","correct":false},{"text":"is shared","correct":false},{"text":"is not shared (https://www.lynda.com/Hadoop-tutorials/Understanding-Java-virtual-machines-JVMs/191942/369545-4.html)","correct":true}],"illustrator":""},{"question":". To create a MapReduce job, what should be coded first?","options":[{"text":"a static job() method","correct":false},{"text":"a Job class and instance (NOT SURE)","correct":true},{"text":"a job() method","correct":false},{"text":"a static Job class","correct":false}],"illustrator":""},{"question":". To connect Hadoop to AWS S3, which client should you use?","options":[{"text":"S3A","correct":true},{"text":"S3N","correct":false},{"text":"S3","correct":false},{"text":"the EMR S3","correct":false}],"illustrator":""},{"question":". HBase works with which type of schema enforcement?","options":[{"text":"schema on write","correct":false},{"text":"no schema","correct":false},{"text":"external schema","correct":false},{"text":"schema on read","correct":true}],"illustrator":""},{"question":". HDFS file are of what type?","options":[{"text":"read-write","correct":false},{"text":"read-only","correct":false},{"text":"write-only","correct":false},{"text":"append-only","correct":true}],"illustrator":""},{"question":". A distributed cache file path can originate from what location?","options":[{"text":"hdfs or top","correct":false},{"text":"http","correct":false},{"text":"hdfs or http","correct":true},{"text":"hdfs","correct":false}],"illustrator":""},{"question":". Which library should you use to perform ETL-type MapReduce jobs?","options":[{"text":"Hive","correct":false},{"text":"Pig","correct":true},{"text":"Impala","correct":false},{"text":"Mahout","correct":false}],"illustrator":""},{"question":". What is the output of the Reducer?","options":[{"text":"a relational table","correct":false},{"text":"an update to the input file","correct":false},{"text":"a single, combined list","correct":false},{"text":"a set of <key, value> pairs","correct":true}],"illustrator":""},{"question":". To optimize a Mapper, what should you perform first?","options":[{"text":"Override the default Partitioner.","correct":false},{"text":"Skip bad records.","correct":false},{"text":"Break up Mappers that do more than one task into multiple Mappers.","correct":false},{"text":"Combine Mappers that do one task into large Mappers.","correct":false}],"illustrator":""},{"question":". When implemented on a public cloud, with what does Hadoop processing interact?","options":[{"text":"files in object storage","correct":true},{"text":"graph data in graph databases","correct":false},{"text":"relational data in managed RDBMS systems","correct":false},{"text":"JSON data in NoSQL databases","correct":false}],"illustrator":""},{"question":". In the Hadoop system, what administrative mode is used for maintenance?","options":[{"text":"data mode","correct":false},{"text":"safe mode","correct":true},{"text":"single-user mode","correct":false},{"text":"pseudo-distributed mode","correct":false}],"illustrator":""},{"question":". In what format does RecordWriter write an output file?","options":[{"text":"<key, value> pairs","correct":true},{"text":"keys","correct":false},{"text":"values","correct":false},{"text":"<value, key> pairs","correct":false}],"illustrator":""},{"question":". To what does the Mapper map input key/value pairs?","options":[{"text":"an average of keys for values","correct":false},{"text":"a sum of keys for values","correct":false},{"text":"a set of intermediate key/value pairs","correct":true},{"text":"a set of final key/value pairs","correct":false}],"illustrator":""},{"question":". Which Hive query returns the first 1,000 values?","options":[{"text":"SELECT…WHERE value = 1000","correct":false},{"text":"SELECT … LIMIT 1000","correct":true},{"text":"SELECT TOP 1000 …","correct":false},{"text":"SELECT MAX 1000…","correct":false}],"illustrator":""},{"question":". To implement high availability, how many instances of the master node should you configure?","options":[{"text":"one","correct":false},{"text":"zero","correct":false},{"text":"shared","correct":false},{"text":"two or more (https://data-flair.training/blogs/hadoop-high-availability-tutorial)","correct":true}],"illustrator":""},{"question":". Hadoop 2.x and later implement which service as the resource coordinator?","options":[{"text":"kubernetes","correct":false},{"text":"JobManager","correct":false},{"text":"JobTracker","correct":false},{"text":"YARN","correct":true}],"illustrator":""},{"question":". In MapReduce, **\\_** have \\_","options":[{"text":"tasks; jobs","correct":false},{"text":"jobs; activities","correct":false},{"text":"jobs; tasks","correct":true},{"text":"activities; tasks","correct":false}],"illustrator":""},{"question":". What type of software is Hadoop Common?","options":[{"text":"database","correct":false},{"text":"distributed computing framework","correct":true},{"text":"operating system","correct":false},{"text":"productivity tool","correct":false}],"illustrator":""},{"question":". If no reduction is desired, you should set the numbers of \\_ tasks to zero","options":[{"text":"combiner","correct":false},{"text":"reduce","correct":true},{"text":"mapper","correct":false},{"text":"intermediate","correct":false}],"illustrator":""},{"question":". MapReduce applications use which of these classes to report their statistics?","options":[{"text":"mapper","correct":false},{"text":"reducer","correct":false},{"text":"combiner","correct":false},{"text":"counter","correct":true}],"illustrator":""},{"question":". \\_ is the query language, and \\_ is storage for NoSQL on Hadoop","options":[{"text":"HDFS; HQL","correct":false},{"text":"HQL; HBase","correct":true},{"text":"HDFS; SQL","correct":false},{"text":"SQL; HBase","correct":false}],"illustrator":""},{"question":". MapReduce 1.0 \\_ YARN","options":[{"text":"does not include","correct":true},{"text":"is the same thing as","correct":false},{"text":"includes","correct":false},{"text":"replaces","correct":false}],"illustrator":""},{"question":". Which type of Hadoop node executes file system namespace operations like opening, closing, and renaming files and directories?","options":[{"text":"ControllerNode","correct":false},{"text":"DataNode","correct":false},{"text":"MetadataNode","correct":false},{"text":"NameNode","correct":true}],"illustrator":""},{"question":". HQL queries produce which job types?","options":[{"text":"Impala","correct":false},{"text":"MapReduce","correct":false},{"text":"Spark","correct":false},{"text":"Pig","correct":false}],"illustrator":""},{"question":". Suppose you are trying to finish a Pig script that converts text in the input string to uppercase. What code is needed on line 2 below?","options":[{"text":"as (text:CHAR[]); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);","correct":false},{"text":"as (text:CHARARRAY); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);","correct":true},{"text":"as (text:CHAR[]); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);","correct":false},{"text":"as (text:CHARARRAY); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);","correct":false}],"illustrator":""},{"question":". In a MapReduce job, which phase runs after the Map phase completes?","options":[{"text":"Combiner","correct":true},{"text":"Reducer","correct":false},{"text":"Map2","correct":false},{"text":"Shuffle and Sort","correct":false}],"illustrator":""},{"question":". Where would you configure the size of a block in a Hadoop environment?","options":[{"text":"dfs.block.size in hdfs-site.xmls","correct":true},{"text":"orc.write.variable.length.blocks in hive-default.xml","correct":false},{"text":"mapreduce.job.ubertask.maxbytes in mapred-site.xml","correct":false},{"text":"hdfs.block.size in hdfs-site.xml","correct":false}],"illustrator":""},{"question":". Hadoop systems are **\\_** RDBMS systems.","options":[{"text":"replacements for","correct":false},{"text":"not used with","correct":false},{"text":"substitutes for","correct":false},{"text":"additions for","correct":true}],"illustrator":""},{"question":". Which object can be used to distribute jars or libraries for use in MapReduce tasks?","options":[{"text":"distributed cache","correct":true},{"text":"library manager","correct":false},{"text":"lookup store","correct":false},{"text":"registry","correct":false}],"illustrator":""},{"question":". To view the execution details of an Impala query plan, which function would you use ?","options":[{"text":"explain","correct":true},{"text":"query action","correct":false},{"text":"detail","correct":false},{"text":"query plan","correct":false}],"illustrator":""},{"question":". Which feature is used to roll back a corrupted HDFS instance to a previously known good point in time?","options":[{"text":"partitioning","correct":false},{"text":"snapshot","correct":true},{"text":"replication","correct":false},{"text":"high availability","correct":false}],"illustrator":""},{"question":". Hadoop Common is written in which language?","options":[{"text":"C++","correct":false},{"text":"C","correct":false},{"text":"Haskell","correct":false},{"text":"Java","correct":true}],"illustrator":""},{"question":". Which file system does Hadoop use for storage?","options":[{"text":"NAS","correct":false},{"text":"FAT","correct":false},{"text":"HDFS","correct":true},{"text":"NFS","correct":false}],"illustrator":""},{"question":". What kind of storage and processing does Hadoop support?","options":[{"text":"encrypted","correct":false},{"text":"verified","correct":false},{"text":"distributed","correct":true},{"text":"remote","correct":false}],"illustrator":""},{"question":". Hadoop Common consists of which components?","options":[{"text":"Spark and YARN","correct":false},{"text":"HDFS and MapReduce","correct":false},{"text":"HDFS and S3","correct":false},{"text":"Spark and MapReduce","correct":false}],"illustrator":""},{"question":". Most Apache Hadoop committers' work is done at which commercial company?","options":[{"text":"Cloudera","correct":false},{"text":"Microsoft","correct":false},{"text":"Google","correct":false},{"text":"Amazon","correct":false}],"illustrator":""},{"question":". To get information about Reducer job runs, which object should be added?","options":[{"text":"Reporter","correct":false},{"text":"IntReadable","correct":false},{"text":"IntWritable","correct":false},{"text":"Writer","correct":false}],"illustrator":""},{"question":". After changing the default block size and restarting the cluster, to which data does the new size apply?","options":[{"text":"all data","correct":false},{"text":"no data","correct":false},{"text":"existing data","correct":false},{"text":"new data","correct":false}],"illustrator":""},{"question":". Which statement should you add to improve the performance of the following query?","options":[{"text":"GROUP BY","correct":false},{"text":"FILTER","correct":false},{"text":"SUB-SELECT","correct":false},{"text":"SORT","correct":false}],"illustrator":"```\nSELECT\n  c.id,\n  c.name,\n  c.email_preferences.categories.surveys\nFROM customers c;\n```"},{"question":". What custom object should you implement to reduce IO in MapReduce?","options":[{"text":"Comparator","correct":false},{"text":"Mapper","correct":false},{"text":"Combiner","correct":false},{"text":"Reducer","correct":false}],"illustrator":""},{"question":". You can optimize Hive queries using which method?","options":[{"text":"secondary indices","correct":false},{"text":"summary statistics","correct":false},{"text":"column-based statistics","correct":false},{"text":"a primary key index","correct":false}],"illustrator":""},{"question":". If you are processing a single action on each input, what type of job should you create?","options":[{"text":"partition-only","correct":false},{"text":"map-only","correct":false},{"text":"reduce-only","correct":false},{"text":"combine-only","correct":false}],"illustrator":""},{"question":". The simplest possible MapReduce job optimization is to perform which of these actions?","options":[{"text":"Add more master nodes.","correct":false},{"text":"Implement optimized InputSplits.","correct":false},{"text":"Add more DataNodes.","correct":false},{"text":"Implement a custom Mapper.","correct":false}],"illustrator":""},{"question":". When you implement a custom Writable, you must also define which of these object?","options":[{"text":"a sort policy","correct":false},{"text":"a combiner policy","correct":false},{"text":"a compression policy","correct":false},{"text":"a filter policy","correct":false}],"illustrator":""},{"question":". To copy a file into the Hadoop file system, what command should you use?","options":[{"text":"hadoop fs -copy <fromDir> <toDir>","correct":false},{"text":"hadoop fs -copy <toDir> <fromDir>","correct":false},{"text":"hadoop fs -copyFromLocal <fromDir> <toDir>","correct":true},{"text":"hadoop fs -copyFromLocal <toDir> <fromDir>","correct":false}],"illustrator":""}]